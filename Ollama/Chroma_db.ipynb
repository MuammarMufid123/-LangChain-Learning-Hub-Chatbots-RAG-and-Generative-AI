{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bead633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### building a sample vectordb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae2b534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Today, we stand at the threshold of a new era in artificial intelligence—one defined not by passive assistants that wait for our commands, but by proactive agents that think ahead, learn our preferences, and execute complex tasks on our behalf. At Google I/O 2025, Sundar Pichai and his team unveiled a sweeping vision for these “agentic” AI capabilities, signaling that traditional search and simple chatbots are soon to be relics of the past\\n\\nOne of the marquee announcements was Agent Mode in the Gemini app, a paradigm shift from reactive to directive AI. Instead of merely answering your questions, Gemini can now autonomously plan your day, book meetings, and coordinate multi-step projects—all tailored to your personal data housed in Gmail, Calendar, Maps, and beyond\\n\\nComplementing Agent Mode is Project Mariner, Google’s experimental web-browsing AI agent. Mariner can visit dozens of websites in parallel, synthesize information, and deliver concise, actionable insights—whether you’re researching market trends or troubleshooting a technical issue—saving you hours of manual search\\n\\nUnderpinning these capabilities is the Deep Research agent, which aggregates data not just from the public web but from your own Google Drive and Gmail. By personalizing research reports to your specific context—be it professional analysis or academic study—Deep Research ensures the answers you receive are both timely and deeply relevant\\n\\nGoogle is also integrating these agents directly into Search with a new AI Mode, transforming the classic list-of-links experience into a conversational interface. Users who opt in can enjoy more personalized, context-aware results that anticipate follow-up questions and streamline decision-making\\n\\nIn closing, Google’s new agent AI initiative represents more than incremental improvements—it’s a fundamental reimagining of how we interact with technology. By shifting from reactive queries to proactive, goal-driven assistance, Google is empowering individuals and organizations to achieve more with less effort. As these agents become ever more capable and integrated into our digital lives, the possibilities for innovation are limitless. Thank you.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"speech.txt\", encoding='utf-8')\n",
    "data=loader.load()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5356de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7d8f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_33596\\3981598470.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=OllamaEmbeddings(model=\"llama2:7b\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedding=OllamaEmbeddings(model=\"llama2:7b\")\n",
    "vectordb=Chroma.from_documents(splits, embedding=embedding)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4368b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
